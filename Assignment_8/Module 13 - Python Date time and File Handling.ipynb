{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Assignment Eight\n",
    "\n",
    "### Python Date time and File Handling  - Module 13\n",
    "##### (Files and Exceptions (Reading/writing files, basic file operations))\n",
    "\n",
    "\n",
    "## Module 13: \n",
    "### Exercise\n",
    "\n",
    "##### Module 16 (Date time and File Handling)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## ðŸ’» Exercises: Python Datetime Exercises\n",
    "\n",
    "### Exercises: Level 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-15 19:13:46.475426\n",
      "Day: 15, Month: 1, Year: 2025, Hour: 19, Minute: 13, Timestamp: 1736964826.475426\n",
      "Formatted Date: 01/15/2025, 19:13:46\n",
      "Converted Time: 2019-12-05 00:00:00\n",
      "Time until New Year: 350 days, 4:46:13.524574\n",
      "Time since 1 January 1970: 20103 days, 19:13:46.475426\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "#1 Get the current day, month, year, hour, minute and timestamp from datetime module\n",
    "current_time = datetime.now()\n",
    "print(current_time)\n",
    "day = current_time.day\n",
    "month = current_time.month\n",
    "year = current_time.year\n",
    "hour = current_time.hour\n",
    "minute = current_time.minute\n",
    "timestamp = current_time.timestamp()\n",
    "print(f'Day: {day}, Month: {month}, Year: {year}, Hour: {hour}, Minute: {minute}, Timestamp: {timestamp}')\n",
    "\n",
    "#2 Format the current date using this format: \"%m/%d/%Y, %H:%M:%S\")\n",
    "\n",
    "formatted_date = current_time.strftime(\"%m/%d/%Y, %H:%M:%S\")\n",
    "print(f'Formatted Date: {formatted_date}')\n",
    "\n",
    "\n",
    "#3 Today is 5 December, 2019. Change this time string to time.\n",
    "\n",
    "time_string = \"5 December, 2019\"\n",
    "converted_time = datetime.strptime(time_string, \"%d %B, %Y\")\n",
    "print(f\"Converted Time: {converted_time}\")\n",
    "\n",
    "#4 Calculate the time difference between now and new year.\n",
    "\n",
    "new_year = datetime(year + 1, 1, 1)\n",
    "time_difference = new_year - current_time\n",
    "print(f\"Time until New Year: {time_difference}\")\n",
    "\n",
    "#5 Calculate the time difference between 1 January 1970 and now.\n",
    "before_time = datetime(1970, 1, 1)\n",
    "time_since_epoch = current_time - before_time\n",
    "print(f\"Time since 1 January 1970: {time_since_epoch}\")\n",
    "\n",
    "#6 Think, what can you use the datetime module for? Examples:\n",
    "    # - Time series analysis:\n",
    "    # In fields like finance, weather forecasting, and healthcare, time-stamped data is analyzed for trends, patterns, and forecasting.\n",
    "    # Example: Analyzing stock market data to observe price fluctuations over time.\n",
    "\n",
    "    \n",
    "    # - To get a timestamp of any activities in an application\n",
    "    # Applications log events with timestamps to track activities or debug issues.\n",
    "    # Example: An error log in a web server might use datetime to show when the error occurred.\n",
    "\n",
    "    # - Adding posts on a blog\n",
    "    # Blogs and news articles often include publication timestamps.\n",
    "    # Example: Displaying \"Posted on 15 January 2025 at 14:35\" for a blog post.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ’» File Handling Exercises:\n",
    "\n",
    "### Exercises: Level 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(66, 2400)\n",
      "(83, 2204)\n",
      "(48, 1259)\n",
      "(33, 1375)\n",
      "[('English', 91), ('French', 45), ('Arabic', 25), ('Spanish', 24), ('Portuguese', 9), ('Russian', 9), ('Dutch', 8), ('German', 7), ('Chinese', 5), ('Serbian', 4)]\n",
      "[('English', 91), ('French', 45), ('Arabic', 25)]\n",
      "[{'country': 'China', 'population': 1377422166}, {'country': 'India', 'population': 1295210000}, {'country': 'United States of America', 'population': 323947000}, {'country': 'Indonesia', 'population': 258705000}, {'country': 'Brazil', 'population': 206135893}, {'country': 'Pakistan', 'population': 194125062}, {'country': 'Nigeria', 'population': 186988000}, {'country': 'Bangladesh', 'population': 161006790}, {'country': 'Russian Federation', 'population': 146599183}, {'country': 'Japan', 'population': 126960000}]\n",
      "[{'country': 'China', 'population': 1377422166}, {'country': 'India', 'population': 1295210000}, {'country': 'United States of America', 'population': 323947000}]\n"
     ]
    }
   ],
   "source": [
    "#1 Write a function which count number of lines and number of words in a text. \n",
    "# c:\\Users\\LENOVO\\Desktop\\AREWA DATA SCIENCE TRAINING\\30 Days Python Challenge\\30-Days-of-Python\\data\\obama_speech.txtAll the files are in the data the folder: \n",
    "# a) Read obama_speech.txt file and count number of lines and words \n",
    "# b) Read michelle_obama_speech.txt file and count number of lines and words\n",
    "# c) Read donald_speech.txt file and count number of lines and words\n",
    "# d) Read melina_trump_speech.txt file and count number of lines and words\n",
    "\n",
    "\n",
    "def count_lines_and_words(Path):\n",
    "    with open(Path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "        word_count = sum(len(line.split()) for line in lines)\n",
    "        return len(lines), word_count\n",
    "\n",
    "print(count_lines_and_words('./Data/obama_speech.txt'))\n",
    "print(count_lines_and_words('./Data/michelle_obama_speech.txt'))\n",
    "print(count_lines_and_words('./Data/donald_speech.txt'))\n",
    "print(count_lines_and_words('./Data/melina_trump_speech.txt'))\n",
    "\n",
    "#2 Read the countries_data.json data file in data directory, create a function that finds the ten most spoken languages\n",
    "\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "def most_spoken_languages(filename, top_n):\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        countries = json.load(file)\n",
    "        language_count = Counter(lang for country in countries for lang in country['languages'])\n",
    "        return language_count.most_common(top_n)\n",
    "\n",
    "\n",
    "print(most_spoken_languages(filename='./data/countries_data.json', top_n=10))\n",
    "print(most_spoken_languages(filename='./data/countries_data.json', top_n=3))\n",
    "\n",
    "\n",
    "#3 Read the countries_data.json data file in data directory, create a function that creates a list of the ten most populated countries\n",
    "\n",
    "def most_populated_countries(filename, top_n):\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        countries = json.load(file)\n",
    "        sorted_countries = sorted(countries, key=lambda x: x['population'], reverse=True)\n",
    "        return [{'country': country['name'], 'population': country['population']} for country in sorted_countries[:top_n]]\n",
    "\n",
    "\n",
    "print(most_populated_countries('./data/countries_data.json', 10))\n",
    "print(most_populated_countries('./data/countries_data.json', 3))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ’» File Handling Exercises:\n",
    "\n",
    "### Exercises: Level 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 129), ('and', 113), ('of', 81), ('to', 70), ('our', 67), ('we', 62), ('that', 50), ('a', 48), ('is', 36), ('in', 25)]\n",
      "[('and', 96), ('the', 85), ('to', 84), ('that', 50), ('of', 46), ('a', 41), ('he', 37), ('in', 36), ('my', 28), ('i', 28)]\n",
      "[('the', 65), ('and', 59), ('we', 44), ('will', 40), ('of', 38), ('to', 32), ('our', 30), ('is', 20), ('america', 17), ('for', 13)]\n",
      "[('and', 77), ('to', 55), ('the', 52), ('is', 29), ('i', 28), ('for', 27), ('of', 25), ('that', 24), ('a', 22), ('you', 21)]\n",
      "0.24060150375939848\n",
      "[('the', 868), ('and', 800), ('to', 661), ('i', 658), ('of', 535), ('a', 530), ('is', 381), ('in', 378), ('that', 371), ('you', 367)]\n",
      "179\n",
      "184\n",
      "67\n"
     ]
    }
   ],
   "source": [
    "#4 Extract all incoming email addresses as a list from the email_exchange_big.txt file.\n",
    "import re\n",
    "\n",
    "def extract_emails(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "    emails = re.findall(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}', content)\n",
    "    return emails\n",
    "\n",
    "\n",
    "\n",
    "#3 Find the most common words in the English language.\n",
    "# Call the name of your function find_most_common_words, \n",
    "# it will take two parameters - a string or a file and a positive integer, \n",
    "# indicating the number of words.\n",
    "# Your function will return an array of tuples in descending order. \n",
    "# Check the output\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def find_most_common_words(source, top_n):\n",
    "    if isinstance(source, str) and source.endswith('.txt'):\n",
    "        with open(source, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "    else:\n",
    "        text = source\n",
    "\n",
    "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "    word_count = Counter(words)\n",
    "\n",
    "    return word_count.most_common(top_n)\n",
    "\n",
    "#5 Use the function, find_most_frequent_words to find: \n",
    "# a) The ten most frequent words used in Obama's speech \n",
    "# b) The ten most frequent words used in Michelle's speech \n",
    "# c) The ten most frequent words used in Trump's speech \n",
    "# d) The ten most frequent words used in Melina's speech\n",
    "\n",
    "print(find_most_common_words('./Data/obama_speech.txt', 10))\n",
    "print(find_most_common_words('./Data/michelle_obama_speech.txt', 10))\n",
    "print(find_most_common_words('./Data/donald_speech.txt', 10))\n",
    "print(find_most_common_words('./Data/melina_trump_speech.txt', 10))\n",
    "\n",
    "#6 Write a python application that checks similarity between two texts. \n",
    "# It takes a file or a string as a parameter and it will evaluate the similarity of the two texts. \n",
    "# For instance check the similarity between the transcripts of Michelle's and Melina's speech. \n",
    "# You may need a couple of functions, function to clean the text(clean_text), \n",
    "# function to remove support words(remove_support_words) and finally to check the \n",
    "# similarity(check_text_similarity). \n",
    "# List of stop words are in the data directory\n",
    "\n",
    "def clean_text(text):\n",
    "    return re.sub(r'[^\\w\\s]', '', text.lower())\n",
    "\n",
    "def remove_support_words(text, stop_words_file):\n",
    "    with open(stop_words_file, 'r', encoding='utf-8') as file:\n",
    "        stop_words = set(file.read().split())\n",
    "    return ' '.join(word for word in text.split() if word not in stop_words)\n",
    "\n",
    "def check_text_similarity(file1, file2, stop_words_file):\n",
    "    with open(file1, 'r', encoding='utf-8') as f1, open(file2, 'r', encoding='utf-8') as f2:\n",
    "        text1 = remove_support_words(clean_text(f1.read()), stop_words_file)\n",
    "        text2 = remove_support_words(clean_text(f2.read()), stop_words_file)\n",
    "        words1, words2 = set(text1.split()), set(text2.split())\n",
    "        return len(words1 & words2) / len(words1 | words2)\n",
    "\n",
    "print(check_text_similarity('./Data/michelle_obama_speech.txt', './Data/melina_trump_speech.txt', './Data/stop_words.txt'))\n",
    "\n",
    "\n",
    "#6 Find the 10 most repeated words in the romeo_and_juliet.txt\n",
    "\n",
    "print(find_most_common_words('./data/romeo_and_juliet.txt', 10))\n",
    "\n",
    "#7 Read the hacker news csv file and find out: \n",
    "# a) Count the number of lines containing python or Python \n",
    "# b) Count the number lines containing JavaScript, javascript or Javascript \n",
    "# c) Count the number lines containing Java and not JavaScript\n",
    "\n",
    "def count_lines_containing(file_path, keywords, exclude=None):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "    count = 0\n",
    "    for line in lines:\n",
    "        if any(keyword.lower() in line.lower() for keyword in keywords):\n",
    "            if exclude and any(excluded.lower() in line.lower() for excluded in exclude):\n",
    "                continue\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "print(count_lines_containing('./data/hacker_news.csv', ['python']))\n",
    "print(count_lines_containing('./data/hacker_news.csv', ['javascript']))\n",
    "print(count_lines_containing('./data/hacker_news.csv', ['java'], exclude=['javascript']))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
